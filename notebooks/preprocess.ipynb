{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmelt\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\jmelt\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 12 from C header, got 20 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                    8065\n",
      "user_key                      0\n",
      "created_at                   21\n",
      "created_str                  21\n",
      "retweet_count            145399\n",
      "retweeted                145399\n",
      "favorite_count           145399\n",
      "text                         21\n",
      "tweet_id                   2314\n",
      "source                   145398\n",
      "hashtags                      0\n",
      "expanded_urls                 0\n",
      "posted                        0\n",
      "mentions                      0\n",
      "retweeted_status_id      163802\n",
      "in_reply_to_status_id    202892\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets = pd.read_csv('../tweets/tweets.csv', header=0)\n",
    "print(tweets.isna().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203430 entries, 0 to 203429\n",
      "Data columns (total 6 columns):\n",
      "user_key    203430 non-null object\n",
      "date        203430 non-null object\n",
      "text        203430 non-null object\n",
      "tweet_id    201116 non-null float64\n",
      "hashtags    203430 non-null object\n",
      "mentions    203430 non-null object\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 5.4+ MB\n",
      "\n",
      "Unique account keys: 453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.drop(['user_id', 'created_at', 'retweet_count', 'retweeted',\n",
    "             'favorite_count', 'source', 'expanded_urls', 'posted',\n",
    "             'retweeted_status_id', 'in_reply_to_status_id'],\n",
    "            axis=1, inplace=True)\n",
    "\n",
    "# Remove rows with NaN text field\n",
    "tweets.dropna(subset=['text'], inplace=True)\n",
    "tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tweets.rename(columns={'created_str': 'date'}, inplace=True)\n",
    "\n",
    "tweets.info()\n",
    "print()\n",
    "print('Unique account keys:', len(tweets.user_key.unique()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding hashtags in tweets\n"
     ]
    }
   ],
   "source": [
    "# Find hashtags in tweets\n",
    "print('Finding hashtags in tweets')\n",
    "tweets['hashtags'] = tweets['text'].apply(lambda text: re.findall('#(\\w+)', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tweet text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmelt\\AppData\\Roaming\\Python\\Python36\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'... '\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\jmelt\\AppData\\Roaming\\Python\\Python36\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Remove URLs\n",
    "    www_exp = r'www.[^ ]+'\n",
    "    http_exp = r'http?s?[^\\s]+'\n",
    "    clean = re.sub('|'.join((www_exp, http_exp)), '', text)\n",
    "\n",
    "    # Remove HTML encoded text (ampersand)\n",
    "    soup = BeautifulSoup(clean, 'lxml')\n",
    "    clean = soup.get_text()\n",
    "    try:\n",
    "        clean = clean.encode().decode().replace(u'\\ufffd', '?')\n",
    "    except UnicodeEncodeError or UnicodeDecodeError:\n",
    "        clean = clean\n",
    "\n",
    "    # Split 'not' contractions\n",
    "    contraction_dict = {'can\\'t': 'can not', 'won\\'t': 'will not',\n",
    "                        'isn\\'t': 'is not', 'aren\\'t': 'are not',\n",
    "                        'wasn\\'t': 'was not', 'weren\\'t': 'were not',\n",
    "                        'haven\\'t': 'have not', 'hasn\\'t': 'has not',\n",
    "                        'wouldn\\'t': 'would not', 'don\\'t': 'do not',\n",
    "                        'doesn\\'t': 'does not', 'didn\\'t': 'did not',\n",
    "                        'couldn\\'t': 'could not', 'shouldn\\'t': 'should not',\n",
    "                        'mightn\\'t': 'might not', 'mustn\\'t': 'must not',\n",
    "                        'had\\'t': 'had not'}\n",
    "    contraction_exp = re.compile(r'\\b(' + '|'.join(contraction_dict.keys()) + r')\\b')\n",
    "    clean = contraction_exp.sub(lambda x: contraction_dict[x.group()], clean)\n",
    "\n",
    "    # Remove @ mentions and hashtags\n",
    "    # at_exp = r'@[A-Za-z0-9_]+'\n",
    "    # hashtag_exp = r'#[A-Za-z0-9_]+'\n",
    "    # clean = re.sub('|'.join((at_exp, hashtag_exp)), '', clean)\n",
    "\n",
    "    # Remove non-letter chars, 'RT'/'MT' from retweets, enclitics from split contractions\n",
    "    clean = re.sub('[^a-zA-Z0-9]', ' ', clean)\n",
    "    tails = [r'\\bRT\\b', r'\\bMT\\b', r'\\bve\\b', r'\\bre\\b', r'\\bll\\b']\n",
    "    clean = re.sub('|'.join(tails), '', clean)\n",
    "    # Convert to lower case\n",
    "    clean = clean.lower()\n",
    "\n",
    "    return ' '.join([word for word in clean.split(' ') if word is not ''])\n",
    "\n",
    "\n",
    "# Apply preprocessing to tweet text\n",
    "print('Cleaning tweet text')\n",
    "tweets['clean_text'] = tweets['text'].apply(clean_text)\n",
    "tweets.dropna(subset=['clean_text'], inplace=True)\n",
    "tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmelt\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\jmelt\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 12 from C header, got 20 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 0 tweets\n",
      "Lemmatized 10000 tweets\n",
      "Lemmatized 20000 tweets\n",
      "Lemmatized 30000 tweets\n",
      "Lemmatized 40000 tweets\n",
      "Lemmatized 50000 tweets\n",
      "Lemmatized 60000 tweets\n",
      "Lemmatized 70000 tweets\n",
      "Lemmatized 80000 tweets\n",
      "Lemmatized 90000 tweets\n",
      "Lemmatized 100000 tweets\n",
      "Lemmatized 110000 tweets\n",
      "Lemmatized 120000 tweets\n",
      "Lemmatized 130000 tweets\n",
      "Lemmatized 140000 tweets\n",
      "Lemmatized 150000 tweets\n",
      "Lemmatized 160000 tweets\n",
      "Lemmatized 170000 tweets\n",
      "Lemmatized 180000 tweets\n",
      "Lemmatized 190000 tweets\n",
      "Lemmatized 200000 tweets\n",
      "Lemmatized 203429 tweets\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203430 entries, 0 to 203429\n",
      "Data columns (total 8 columns):\n",
      "user_key      203430 non-null object\n",
      "date          203430 non-null object\n",
      "text          203430 non-null object\n",
      "tweet_id      201116 non-null float64\n",
      "hashtags      203430 non-null object\n",
      "mentions      203430 non-null object\n",
      "clean_text    203430 non-null object\n",
      "lemmas        203430 non-null object\n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet_text = list(tweets.clean_text)\n",
    "\n",
    "# Tokenize tweets and remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "my_stops = ['go', 'be', 'also', 'get', 'do', 'thing', 'use', 'let', 'would', 'say', 'could', 'yet']\n",
    "stop_words.update(my_stops)\n",
    "\n",
    "print('Tokenizing and removing stop words')\n",
    "tweet_text = [[word for word in word_tokenize(tweet) if word not in stop_words and len(word) > 2]\n",
    "              for tweet in tweet_text]\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=('NOUN', 'ADJ', 'VERB', 'ADV')):\n",
    "    lemmas = []\n",
    "    for i, tweet in enumerate(texts):\n",
    "        if i % 10000 == 0 or i == len(tweets)-1:\n",
    "            print('Lemmatized {} tweets'.format(i))\n",
    "        lemmas.append([token.lemma_ for token in nlp(' '.join(tweet))\n",
    "                       if token.pos_ in allowed_postags and token.lemma_ not in stop_words])\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "tweet_lemmas = lemmatization(tweet_text)\n",
    "tweets['lemmas'] = [' '.join(words) for words in tweet_lemmas]\n",
    "tweets.dropna(subset=['lemmas'], inplace=True)\n",
    "tweets.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print()\n",
    "tweets.info()\n",
    "\n",
    "tweets.to_csv('../tweets/tweets_clean.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
